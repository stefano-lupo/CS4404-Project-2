{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "from util import (\n",
    "    readData,\n",
    "    createDesignMatrix,\n",
    "    createLabelVector,\n",
    "    printM,\n",
    "    splitData7030,\n",
    "    splitUpDataCrossVal,\n",
    "    featureNormalize,\n",
    "    multiclassToBinaryClass,\n",
    "    ACTIVE_DATASET,\n",
    "    TRAINING_PARAMS\n",
    ")\n",
    "\n",
    "from plotCallback import (PlotLossAccuracy)\n",
    "\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over sample the minority classes\n",
    "# NOTE: SHOULD OVERSAMPLE AFTER SPLITTING DATA NOT BEFORE\n",
    "# OTHERWISE JUST LEARNING THE VALIDATION SET TOO\n",
    "def splitAndOverSample(X, y, numDataSplits=None, crossValIndex=None):\n",
    "\n",
    "    # Split the data\n",
    "    if numDataSplits:\n",
    "        xTrain, yTrain, xVal, yVal = splitUpDataCrossVal(X, y, numDataSplits, crossValIndex)\n",
    "    else:\n",
    "        xTrain, yTrain, xVal, yVal = splitData7030(X, y)\n",
    "\n",
    "    # If no sampling to be done\n",
    "    if not TRAINING_PARAMS['BALANCE_SAMPLING']:\n",
    "\n",
    "        ## Binary encode the classes data\n",
    "        yTrain = to_categorical(yTrain, num_classes=NUM_CLASSES)\n",
    "        yVal = to_categorical(yVal, num_classes=NUM_CLASSES)\n",
    "        print(\"After binary encoding y: \", yTrain.shape)\n",
    "        return  xTrain, yTrain, xVal, yVal\n",
    "    else:\n",
    "        counts = Counter(yTrain)\n",
    "\n",
    "        if TRAINING_PARAMS['BALANCE_SAMPLING'] == 'OVER':\n",
    "\n",
    "            # Define oversampling amounts\n",
    "            ratioDict = {3: max(20, counts[3]), 4: max(80, counts[4]), 5: counts[5],\n",
    "                         6: counts[6], 7: counts[7], 8: max(80, counts[8]), 9: max(20, counts[9])}\n",
    "\n",
    "            # Oversample the training data\n",
    "            xTrainOS, yTrainOS = RandomOverSampler(random_state=0, ratio=ratioDict).fit_sample(xTrain, yTrain)\n",
    "            # xTrainOS, yTrainOS = SMOTE(k_neighbors=3, ratio=ratioDict).fit_sample(xTrain, yTrain)\n",
    "            # xTrainOS, yTrainOS = ADASYN(n_neighbors=4, ratio=ratioDict).fit_sample(xTrain, yTrain)\n",
    "\n",
    "            print('Oversampling')\n",
    "            print('Rating distribution: ', sorted(Counter(yTrainOS).items()))\n",
    "\n",
    "            # Show distribution of classes after over sampling\n",
    "            plt.hist([yTrain, yTrainOS], bins=range(3, 11), align='left', rwidth=0.5, label=['No Oversampling', 'Oversampling'])\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Binary encode\n",
    "            yTrainOS = to_categorical(yTrainOS, num_classes=10)\n",
    "            yVal = to_categorical(yVal, num_classes=10)\n",
    "            print(\"After binary encoding y: \", yTrainOS.shape)\n",
    "\n",
    "            return xTrainOS, yTrainOS, xVal, yVal\n",
    "        else:\n",
    "            ratioDict = {3: counts[3], 4: counts[4], 5: min(counts[5], 400),\n",
    "                         6: min(counts[6], 800), 7: min(counts[7], 400), 8: counts[8], 9: counts[9]}\n",
    "            xTrainUS, yTrainUS = RandomUnderSampler(random_state=0, ratio=ratioDict).fit_sample(xTrain, yTrain)\n",
    "\n",
    "            print('Using Undersampling')\n",
    "            print('Category distribution: ', sorted(Counter(yTrainUS).items()))\n",
    "\n",
    "            # Show distribution of classes after under sampling\n",
    "            plt.hist([yTrain, yTrainUS], bins=range(3, 11), align='left', rwidth=0.5, label=['No Undersampling', 'Undersampling'])\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Binary encode\n",
    "            yTrainUS = to_categorical(yTrainUS, num_classes=10)\n",
    "            yVal = to_categorical(yVal, num_classes=10)\n",
    "            print(\"After binary encoding y: \", yTrainUS.shape)\n",
    "\n",
    "            return xTrainUS, yTrainUS, xVal, yVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using min-max normalization\n",
      "X:  (3896, 4)\n",
      "y:  (3896,)\n",
      "Using min-max normalization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = readData()\n",
    "X = createDesignMatrix(data)\n",
    "X = featureNormalize(X)\n",
    "\n",
    "y = createLabelVector(data)\n",
    "y = np.squeeze(np.asarray(y))\n",
    "print(\"X: \", X.shape)\n",
    "print(\"y: \", y.shape)\n",
    "\n",
    "\n",
    "# print(X)\n",
    "# print(y)\n",
    "# np.savetxt(\"split.csv\", yTrain)\n",
    "\n",
    "data = readData(filename='testData.csv')\n",
    "xTest = createDesignMatrix(data)\n",
    "xTest = featureNormalize(xTest)\n",
    "\n",
    "yTest = createLabelVector(data)\n",
    "yTest = np.squeeze(np.asarray(yTest))\n",
    "yTest = to_categorical(yTest, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "pltCallBack = PlotLossAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "validationAccuracies = []\n",
    "k = 10\n",
    "for i in range(k):\n",
    "    \n",
    "    # Create new model for this fold\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', 'categorical_accuracy'])\n",
    "#     model.summary()\n",
    "    \n",
    "    # Split the data\n",
    "    xTrain, yTrain, xVal, yVal = splitAndOverSample(X, y, k, i)\n",
    "#     xTrain, yTrain, xVal, yVal = splitAndOverSample(X, y)\n",
    "    print(\"xTrain: \", xTrain.shape)\n",
    "    print(\"yTrain: \", yTrain.shape)\n",
    "    print(\"xVal: \", xVal.shape)\n",
    "    print(\"yVal: \", yVal.shape)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(xTrain, yTrain, validation_data=(xVal, yVal), epochs=2, callbacks=[pltCallBack])\n",
    "    pltCallBack.show_plots()\n",
    "\n",
    "#     Predict the validation set\n",
    "    predictions = model.predict(xVal)\n",
    "    \n",
    "\n",
    "    # Extract the predicted categories\n",
    "    predictionsCat = np.argmax(predictions, axis=1)\n",
    "    yValcat = np.argmax(yVal, axis=1)\n",
    "    \n",
    "    print(predictionsCat)\n",
    "    print(yVal)\n",
    "    \n",
    "    print(confusion_matrix(yVal, predictionCat))\n",
    "\n",
    "    # Plot histogram of predicted category distribution\n",
    "#     plt.hist(predictionCat, bins=range(3, 11), align='left', rwidth=0.5)\n",
    "#     plt.show()\n",
    "\n",
    "    # print(\"Actual: \")\n",
    "    # print(yVal[0], \"\\n\")\n",
    "\n",
    "    #     # evaluate the model\n",
    "    scores = model.evaluate(xVal, yVal)\n",
    "    print(i, \": Validation Accuracy = \", scores[1])\n",
    "    validationAccuracies.append(scores[1])\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation Accuracy accross  10 , folds:  0.533664142505\n",
      "Mean Test Accuracy accross  10  folds 0.487824351443\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Validation Accuracy accross \", k, \", folds: \", np.mean(validationAccuracies))\n",
    "\n",
    "\n",
    "testScores = model.evaluate(xTest, yTest)\n",
    "    print(\"Accuracy on Test set = \", testScores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
